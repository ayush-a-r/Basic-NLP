{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMD89ViiJQhBX39sQB6abr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush-a-r/Basic-NLP/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgTzt6-xOOxH",
        "outputId": "6c8f614a-bc31-46e4-f733-86fd04af6926"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics discovered by LDA:\n",
            "(0, '0.155*\"learning\" + 0.121*\"machine\" + 0.086*\"ai\" + 0.052*\"subset\" + 0.052*\"algorithms\"')\n",
            "(1, '0.068*\"artificial\" + 0.068*\"industry\" + 0.068*\"transforming\" + 0.068*\"technology\" + 0.068*\"intelligence\"')\n",
            "\n",
            "Document Similarity (cosine): 0.2886751345948129\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "import gensim                               #Imports the gensim library, which is a popular Python library for topic modeling, document similarity, and vector space modeling. It includes the LDA model, which we’ll use for topic modeling.\n",
        "from gensim import corpora                  #Imports the corpora module from Gensim, which provides utilities for handling a corpus of documents. It includes methods for creating a dictionary (mapping words to unique IDs) and for creating document-term matrices.\n",
        "from gensim.models import LdaModel          #Imports the LdaModel class from Gensim. This class is used for training an LDA model on a corpus to discover topics within a collection of documents.\n",
        "from nltk.corpus import stopwords           #Imports the stopwords corpus from NLTK (Natural Language Toolkit), which contains a list of common words (e.g., \"the\", \"is\", \"and\") that are often removed from text during preprocessing.\n",
        "from nltk.tokenize import word_tokenize     #Imports the word_tokenize function from NLTK, which is used for splitting a sentence into individual words or tokens.\n",
        "import nltk             #Imports the main NLTK library to access other utilities like stopwords and tokenizers.\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Download NLTK stopwords (run only once)\n",
        "nltk.download('punkt')       #Downloads the Punkt tokenizer models, which are necessary for word tokenization (splitting sentences into words).\n",
        "nltk.download('stopwords')   #Downloads the stopwords list, which contains a set of common words in English that are typically removed from text before processing.\n",
        "\n",
        "# Sample documents\n",
        "documents = [\n",
        "    \"Artificial intelligence is transforming the technology industry.\",\n",
        "    \"Machine learning and AI are shaping the future of automation.\",\n",
        "    \"Deep learning algorithms are a subset of machine learning.\",\n",
        "    \"Quantum computing will revolutionize industries like AI.\",\n",
        "    \"Healthcare is benefiting from AI and machine learning advances.\",\n",
        "]\n",
        "\n",
        "#This defines a list of sample documents (sentences) that will be used for topic modeling. These sentences are focused on topics related to artificial intelligence (AI) and machine learning (ML).\n",
        "\n",
        "# Preprocess the documents\n",
        "def preprocess(doc):                                    #Defines a function that preprocesses a document (sentence) to prepare it for modeling by removing stopwords and non-alphabetic words.\n",
        "    stop_words = set(stopwords.words('english'))        #Loads the set of English stopwords from NLTK into the stop_words variable. These are words like \"and\", \"the\", \"is\", etc., that generally do not carry important meaning for topic modeling.\n",
        "    tokens = word_tokenize(doc.lower())                 #Tokenizes the input doc (document) into individual words (tokens) and converts all words to lowercase using .lower() to ensure uniformity (e.g., \"AI\" and \"ai\" will be treated as the same).\n",
        "    return [word for word in tokens if word.isalpha() and word not in stop_words]    #Filters out any tokens that are non-alphabetic (such as punctuation or numbers) and any stopwords. It returns a list of meaningful words (tokens).\n",
        "\n",
        "processed_docs = [preprocess(doc) for doc in documents]               #Applies the preprocess() function to each document in the documents list. This results in a list of tokenized, lowercased, stopword-free words for each document.\n",
        "\n",
        "# Create a dictionary and document-term matrix\n",
        "dictionary = corpora.Dictionary(processed_docs)                          #Creates a dictionary using the processed documents. The dictionary maps each unique word (token) to a unique ID. This is an essential step before building a document-term matrix (DTM).\n",
        "doc_term_matrix = [dictionary.doc2bow(doc) for doc in processed_docs]    #Converts each preprocessed document into a bag-of-words representation using dictionary.doc2bow(). The doc2bow function converts each document into a list of tuples, where each tuple represents a word ID and its frequency in the document.\n",
        "\n",
        "# Train the LDA model (specifying 2 topics)\n",
        "lda_model = LdaModel(doc_term_matrix, num_topics=2, id2word=dictionary, passes=15)   #Specifies that the model should discover 2 topics from the documents. id2word=dictionary: The dictionary created earlier is passed to the model to help interpret word IDs.. passes=15: Specifies the number of passes (iterations) over the entire corpus to optimize the model. More passes generally result in better topic quality but take more time.\n",
        "\n",
        "# Print the topics with associated words\n",
        "print(\"Topics discovered by LDA:\")\n",
        "topics = lda_model.print_topics(num_words=5)    #Prints the top 5 words for each discovered topic. This allows you to understand what each topic is about based on the most common words in the topic.\n",
        "for topic in topics:\n",
        "    print(topic)        #Iterates over the topics and prints them out. Each topic consists of a list of words that are highly associated with that topic.\n",
        "\n",
        "# Document similarity (clustering example)\n",
        "doc1_bow = dictionary.doc2bow(preprocess(\"AI and machine learning are advancing rapidly\"))   #Preprocesses the new document, converts it into a bag-of-words format using the dictionary, and stores it in doc1_bow.\n",
        "doc2_bow = dictionary.doc2bow(preprocess(\"Healthcare is benefiting from AI advances\"))       #Preprocesses and converts the second document into a bag-of-words representation, storing it in doc2_bow.\n",
        "\n",
        "similarity = gensim.matutils.cossim(doc1_bow, doc2_bow)   #Computes the cosine similarity between the two document vectors (doc1_bow and doc2_bow). Cosine similarity is a measure of similarity between two vectors based on the cosine of the angle between them. A higher cosine value indicates more similarity.\n",
        "print(\"\\nDocument Similarity (cosine):\", similarity)   #Prints the cosine similarity value between the two documents\n"
      ]
    }
  ]
}
